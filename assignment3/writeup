\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}
\usepackage{color}
\usepackage{listings}
\lstset{ %
language=Python,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}
\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}
\usepackage{graphicx}

\begin{document}
\begin{center}
{\Large CS224D Spring 2015: Homework 3 }

\begin{tabular}{rl}
SUNet ID: & dlchang \\
Name: & Daryl Chang \\
Collaborators: & April Yu, Leon Yao, Derrick Liu
\end{tabular}
\end{center}

By turning in this assignment, I agree by the Stanford honor code and declare
that all of this is my own work.

\section*{Problem 1}
\subsection*{Part a}
\subsubsection*{Node 1}
$$\delta_3 = \hat{y} - y$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(1)} \neq 0\}$$
$$\frac{\partial J}{\partial U} = \delta_3 h^{{(1)}T}$$
$$\frac{\partial J}{\partial b^{(s)}} = \delta_3$$
$$\frac{\partial J}{\partial W^{(1)}} = \delta_2 \begin{bmatrix} h^{(1)}_{left} \\ h^{(1)}_{right} \end{bmatrix}^T$$
$$\frac{\partial J}{\partial b^{(1)}} = \delta_2$$
$$\frac{\partial J}{\partial L_i} = 0$$ 

\subsubsection*{Node 2}
$$\delta_3 = \hat{y} - y$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(1)} \neq 0\} + W^T_{[:, :d]} \delta_{above} \circ 1\{h^{(1)} \neq 0\}$$
$$\frac{\partial J}{\partial U} = \delta_3 h^{{(1)}T}$$
$$\frac{\partial J}{\partial b^{(s)}} = \delta_3$$
$$\frac{\partial J}{\partial W^{(1)}} = \delta_2 \begin{bmatrix} L_7 \\ L_{145} \end{bmatrix}^T$$
$$\frac{\partial J}{\partial b^{(1)}} = \delta_2$$
$$\frac{\partial J}{\partial L_i} = 0$$ 

\subsubsection*{Node 3}
$$\delta_3 = \hat{y} - y$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(1)} \neq 0\} + W^T_{[:, d:]} \delta_{above} \circ 1\{h^{(1)} \neq 0\}$$
$$\frac{\partial J}{\partial U} = \delta_3 h^{{(1)}T}$$
$$\frac{\partial J}{\partial b^{(s)}} = \delta_3$$
$$\frac{\partial J}{\partial W^{(1)}} = \delta_2 \begin{bmatrix} L_{29} \\ L_{430} \end{bmatrix}^T$$
$$\frac{\partial J}{\partial b^{(1)}} = \delta_2$$
$$\frac{\partial J}{\partial L_i} = 0$$ 

\subsubsection*{Leaves}
$$\delta_3 = \hat{y} - y$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(1)} \neq 0\} + W^T_{[:, :d]} \delta_{above} \circ 1\{h^{(1)} \neq 0\} \mbox{\ if left child}$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(1)} \neq 0\} + W^T_{[:, d:]} \delta_{above} \circ 1\{h^{(1)} \neq 0\} \mbox{\ if right child}$$
$$\frac{\partial J}{\partial U} = \delta_3 L_i^T$$
$$\frac{\partial J}{\partial b^{(s)}} = \delta_3$$
$$\frac{\partial J}{\partial W^{(1)}} = 0$$
$$\frac{\partial J}{\partial b^{(1)}} = 0$$
$$\frac{\partial J}{\partial L_i} = \delta_2$$ 

\subsection*{Part b}
See code.

\subsection*{Part c}
\subsubsection*{Subpart a}
\includegraphics[width=120mm]{images/train_dev_accuracies.png}  


Best dev set accuracy occurred at 41 epochs.

\subsubsection*{Subpart b}
The dev accuracy starts to decrease after a certain point because the neural network overfits the training data, resulting in a loss of generalization.

\subsubsection*{Subpart c}
\begin{figure}
\begin{center}
\includegraphics[width=120mm]{images/conf_epochs_41_train.png} 
\caption{Confusion matrix on training data}
\end{center}
\end{figure}
 
\begin{figure}
\begin{center}
\includegraphics[width=120mm]{images/conf_epochs_41_dev.png} 
\caption{Confusion matrix on dev data}
\end{center}
\end{figure}

\newpage
\subsubsection*{Subpart d}
\includegraphics[width=120mm]{images/wvecdim.png} 

\section*{Problem 2}
\subsection*{Part a}
\subsubsection*{Node 1}
$$\delta_3 = \hat{y} - y$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(2)} \neq 0 \}$$
$$\delta_1 = W^{(2)T}\delta_2 \circ 1\{h^{(1)} \neq 0 \}$$
$$\frac{\partial J}{\partial U} = \delta_3 h^{(2)T}$$
$$\frac{\partial J}{\partial b^{(s)}} = \delta_3$$
$$\frac{\partial J}{\partial W^{(1)}} = \delta_1  \begin{bmatrix} h^{(1)}_{Left} \\ h^{(1)}_{Right} \end{bmatrix}^T$$
$$\frac{\partial J}{\partial b^{(1)}} = \delta_1$$
$$\frac{\partial J}{\partial W^{(2)}} = \delta_2 h^{(1)T}$$
$$\frac{\partial J}{\partial b^{(2)}} = \delta_2$$
$$\frac{\partial J}{\partial L_i} = 0$$

\subsubsection*{Node 2}
$$\delta_3 = \hat{y} - y$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(2)} \neq 0 \}$$
$$\delta_1 = W^{(2)T}\delta_2 \circ 1\{h^{(1)} \neq 0 \} + W^{(1)T}_{[:,:d]} \delta_{above} \circ 1\{h^{(1)} \neq 0 \}$$
$$\frac{\partial J}{\partial U} = \delta_3 h^{(2)T}$$
$$\frac{\partial J}{\partial b^{(s)}} = \delta_3$$
$$\frac{\partial J}{\partial W^{(1)}} = \delta_1  \begin{bmatrix} L_7 \\ L_{145} \end{bmatrix}^T$$
$$\frac{\partial J}{\partial b^{(1)}} = \delta_1$$
$$\frac{\partial J}{\partial W^{(2)}} = \delta_2 h^{(1)T}$$
$$\frac{\partial J}{\partial b^{(2)}} = \delta_2$$
$$\frac{\partial J}{\partial L_i} = 0$$

\subsubsection*{Node 3}
$$\delta_3 = \hat{y} - y$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(2)} \neq 0 \}$$
$$\delta_1 = W^{(2)T}\delta_2 \circ 1\{h^{(1)} \neq 0 \} + W^{(1)T}_{[:,d:]} \delta_{above} \circ 1\{h^{(1)} \neq 0 \}$$
$$\frac{\partial J}{\partial U} = \delta_3 h^{(2)T}$$
$$\frac{\partial J}{\partial b^{(s)}} = \delta_3$$
$$\frac{\partial J}{\partial W^{(1)}} = \delta_1  \begin{bmatrix} L_{29} \\ L_{430} \end{bmatrix}^T$$
$$\frac{\partial J}{\partial b^{(1)}} = \delta_1$$
$$\frac{\partial J}{\partial W^{(2)}} = \delta_2 h^{(1)T}$$
$$\frac{\partial J}{\partial b^{(2)}} = \delta_2$$
$$\frac{\partial J}{\partial L_i} = 0$$

\subsubsection*{Leaves}
$$\delta_3 = \hat{y} - y$$
$$\delta_2 = U^T \delta_3 \circ 1\{h^{(2)} \neq 0 \}$$
$$\delta_1 = W^{(2)T}\delta_2 \circ 1\{L_i \neq 0 \} + W^{(1)T}_{[:,:d]} \delta_{above} \circ 1\{ L_i \neq 0 \} \mbox{\ if left child}$$
$$\delta_1 = W^{(2)T}\delta_2 \circ 1\{L_i \neq 0 \} + W^{(1)T}_{[:,d:]} \delta_{above} \circ 1\{ L_i \neq 0 \} \mbox{\ if right child}$$
$$\frac{\partial J}{\partial U} = \delta_3 h^{(2)T}$$
$$\frac{\partial J}{\partial b^{(s)}} = \delta_3$$
$$\frac{\partial J}{\partial W^{(1)}} = 0$$
$$\frac{\partial J}{\partial b^{(1)}} = 0$$
$$\frac{\partial J}{\partial W^{(2)}} = \delta_2 L_i^T$$
$$\frac{\partial J}{\partial b^{(2)}} = \delta_2$$
$$\frac{\partial J}{\partial L_i} = W^{(1)T}_{[:,:d]} \delta_{above} \circ 1\{ L_i \neq 0 \} + W^{(2)T} \delta^2 \circ 1\{L_i \neq 0\} \mbox{\ if left child}$$
$$\frac{\partial J}{\partial L_i} = W^{(1)T}_{[:,d:]} \delta_{above} \circ 1\{ L_i \neq 0 \} + W^{(2)T} \delta^2 \circ 1\{L_i \neq 0\} \mbox{\ if right child}$$

\subsection*{Part b}
See code.

\subsection*{Part c}
\subsubsection*{Subpart a}
\includegraphics[width=120mm]{images/RNN2_train_dev_accuracies.png} 
\\ The best dev accuracy occurred at 6 epochs.

\subsubsection*{Subpart b}
\begin{figure}
\begin{center}
\includegraphics[width=120mm]{images/RNN2_conf_epochs_6_train.png} 
\caption{Confusion matrix on training data}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=120mm]{images/RNN2_conf_epochs_6_dev.png} 
\caption{Confusion matrix on dev data}
\end{center}
\end{figure}

\newpage
\subsubsection*{Subpart c}
The RNN does slightly better than the original one.  This is most likely because the added layer allows the network to learn higher-order features, which in turn increases accuracy.

\subsubsection*{Subpart d}
\includegraphics[width=120mm]{images/middledim.png}

\subsection*{Part d}
One possible way to improve the performance of the neural network is to make it into a recursive autoencoder (i.e. to base the structure of the network on the syntactic parse of the sentence).  The recursive autoencoder would essentially encode groups of words into a hidden representation until the entire sentence is encoded; it minimizes the reconstruction error.This would account for the syntactic structure of the sentence, which should in turn increase the performance of the network.

\end{document}